[2023-10-15T19:48:45.259+0000] {processor.py:157} INFO - Started process (PID=42) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:48:45.261+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:48:45.269+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:48:45.269+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:48:58.293+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:48:57.692+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:48:58.440+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:49:12.672+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 26.173 seconds
[2023-10-15T19:49:43.577+0000] {processor.py:157} INFO - Started process (PID=53) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:49:43.585+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:49:43.608+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:49:43.597+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:49:47.461+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:49:47.381+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:49:47.466+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:49:47.715+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 4.204 seconds
[2023-10-15T19:50:20.177+0000] {processor.py:157} INFO - Started process (PID=63) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:50:20.191+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:50:20.343+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:50:20.224+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:50:23.054+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:50:23.013+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:50:23.058+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:50:23.101+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 3.022 seconds
[2023-10-15T19:50:53.560+0000] {processor.py:157} INFO - Started process (PID=82) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:50:53.567+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:50:53.570+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:50:53.569+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:50:54.398+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:50:54.382+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:50:54.401+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:50:54.430+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.880 seconds
[2023-10-15T19:51:24.666+0000] {processor.py:157} INFO - Started process (PID=93) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:51:24.667+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:51:24.668+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:51:24.668+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:51:25.245+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:51:25.229+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:51:25.247+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:51:25.281+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.621 seconds
[2023-10-15T19:51:55.843+0000] {processor.py:157} INFO - Started process (PID=105) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:51:55.845+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:51:55.848+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:51:55.847+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:51:57.011+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:51:56.768+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:51:57.022+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:51:58.288+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 2.452 seconds
[2023-10-15T19:52:28.600+0000] {processor.py:157} INFO - Started process (PID=116) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:52:28.602+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:52:28.604+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:52:28.603+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:52:29.297+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:52:29.279+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:52:29.301+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:52:29.355+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.758 seconds
[2023-10-15T19:52:59.523+0000] {processor.py:157} INFO - Started process (PID=127) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:52:59.524+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:52:59.526+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:52:59.525+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:53:00.176+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:53:00.160+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:53:00.180+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:53:00.221+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.703 seconds
[2023-10-15T19:53:30.449+0000] {processor.py:157} INFO - Started process (PID=138) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:53:30.450+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:53:30.451+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:53:30.451+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:53:30.998+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:53:30.984+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:53:31.002+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:53:31.031+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.587 seconds
[2023-10-15T19:54:01.808+0000] {processor.py:157} INFO - Started process (PID=149) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:54:01.809+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:54:01.812+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:54:01.811+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:54:02.387+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:54:02.373+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:54:02.389+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:54:02.418+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.615 seconds
[2023-10-15T19:54:32.687+0000] {processor.py:157} INFO - Started process (PID=160) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:54:32.688+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:54:32.689+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:54:32.689+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:54:33.202+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:54:33.188+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:54:33.204+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:54:33.234+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.551 seconds
[2023-10-15T19:55:03.489+0000] {processor.py:157} INFO - Started process (PID=171) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:55:03.490+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:55:03.492+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:55:03.491+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:55:04.057+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:55:04.043+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:55:04.059+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:55:04.096+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.610 seconds
[2023-10-15T19:55:34.355+0000] {processor.py:157} INFO - Started process (PID=182) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:55:34.356+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:55:34.358+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:55:34.357+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:55:34.978+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:55:34.964+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:55:34.980+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:55:35.003+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.654 seconds
[2023-10-15T19:56:05.258+0000] {processor.py:157} INFO - Started process (PID=193) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:56:05.259+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:56:05.260+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:56:05.260+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:56:05.925+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:56:05.907+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:56:05.928+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:56:05.963+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.711 seconds
[2023-10-15T19:56:36.197+0000] {processor.py:157} INFO - Started process (PID=204) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:56:36.198+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:56:36.199+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:56:36.198+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:56:36.737+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:56:36.717+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:56:36.739+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:56:36.772+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.579 seconds
[2023-10-15T19:57:07.052+0000] {processor.py:157} INFO - Started process (PID=215) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:57:07.053+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:57:07.055+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:57:07.055+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:57:07.652+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:57:07.638+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:57:07.655+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:57:07.690+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.643 seconds
[2023-10-15T19:57:37.937+0000] {processor.py:157} INFO - Started process (PID=227) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:57:37.938+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:57:37.940+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:57:37.940+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:57:38.486+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:57:38.471+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:57:38.489+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:57:38.514+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.583 seconds
[2023-10-15T19:58:08.738+0000] {processor.py:157} INFO - Started process (PID=238) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:58:08.740+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:58:08.742+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:58:08.741+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:58:09.428+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:58:09.409+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:58:09.433+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:58:09.490+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.758 seconds
[2023-10-15T19:58:39.691+0000] {processor.py:157} INFO - Started process (PID=249) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:58:39.692+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:58:39.695+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:58:39.694+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:58:40.428+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:58:40.409+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:58:40.432+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:58:40.474+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.791 seconds
[2023-10-15T19:59:10.918+0000] {processor.py:157} INFO - Started process (PID=260) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:59:10.919+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:59:10.921+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:59:10.921+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:59:11.676+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:59:11.661+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:59:11.679+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:59:11.720+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.808 seconds
[2023-10-15T19:59:41.883+0000] {processor.py:157} INFO - Started process (PID=271) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T19:59:41.884+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T19:59:41.886+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:59:41.886+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:59:42.495+0000] {logging_mixin.py:151} INFO - [2023-10-15T19:59:42.476+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T19:59:42.499+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T19:59:42.535+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.656 seconds
[2023-10-15T20:00:12.794+0000] {processor.py:157} INFO - Started process (PID=283) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:00:12.796+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:00:12.797+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:00:12.797+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:00:13.299+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:00:13.286+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:00:13.301+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:00:13.332+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.542 seconds
[2023-10-15T20:00:43.573+0000] {processor.py:157} INFO - Started process (PID=294) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:00:43.575+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:00:43.577+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:00:43.576+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:00:44.108+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:00:44.095+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:00:44.110+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:00:44.135+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.570 seconds
[2023-10-15T20:01:14.393+0000] {processor.py:157} INFO - Started process (PID=306) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:01:14.394+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:01:14.395+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:01:14.395+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:01:14.916+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:01:14.903+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:01:14.919+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:01:14.949+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.560 seconds
[2023-10-15T20:01:45.208+0000] {processor.py:157} INFO - Started process (PID=317) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:01:45.209+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:01:45.210+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:01:45.210+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:01:45.741+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:01:45.725+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:01:45.743+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:01:45.778+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.574 seconds
[2023-10-15T20:02:16.039+0000] {processor.py:157} INFO - Started process (PID=328) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:02:16.040+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:02:16.041+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:02:16.041+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:02:16.539+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:02:16.527+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:02:16.542+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:02:16.577+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.543 seconds
[2023-10-15T20:02:46.836+0000] {processor.py:157} INFO - Started process (PID=339) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:02:46.837+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:02:46.839+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:02:46.838+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:02:47.355+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:02:47.339+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:02:47.363+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:02:47.391+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.559 seconds
[2023-10-15T20:03:17.745+0000] {processor.py:157} INFO - Started process (PID=350) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:03:17.746+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:03:17.748+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:03:17.747+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:46:55.338+0000] {processor.py:157} INFO - Started process (PID=42) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:46:55.341+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:46:55.345+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:46:55.345+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:47:02.877+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:47:02.795+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:47:02.885+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:47:02.985+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 7.662 seconds
[2023-10-15T20:47:38.185+0000] {processor.py:157} INFO - Started process (PID=53) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:47:38.244+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:47:38.322+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:47:38.308+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:47:49.939+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:47:49.857+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:47:49.944+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:47:50.062+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 14.277 seconds
[2023-10-15T20:48:20.893+0000] {processor.py:157} INFO - Started process (PID=64) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:48:20.897+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:48:20.925+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:48:20.899+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:48:31.024+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:48:30.681+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:48:31.028+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:48:31.146+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 10.366 seconds
[2023-10-15T20:49:01.445+0000] {processor.py:157} INFO - Started process (PID=82) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:49:01.451+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:49:01.453+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:49:01.452+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:49:02.168+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:49:02.152+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:49:02.170+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:49:02.233+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.795 seconds
[2023-10-15T20:49:32.540+0000] {processor.py:157} INFO - Started process (PID=93) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:49:32.545+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:49:32.549+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:49:32.548+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:49:33.326+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:49:33.257+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:49:33.330+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:49:33.786+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 1.272 seconds
[2023-10-15T20:50:04.093+0000] {processor.py:157} INFO - Started process (PID=104) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:50:04.095+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:50:04.097+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:50:04.096+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:50:04.767+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:50:04.752+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:50:04.770+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:50:04.807+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.720 seconds
[2023-10-15T20:50:35.064+0000] {processor.py:157} INFO - Started process (PID=115) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:50:35.067+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:50:35.069+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:50:35.068+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:50:35.587+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:50:35.571+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:50:35.591+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:50:35.629+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.570 seconds
[2023-10-15T20:51:05.909+0000] {processor.py:157} INFO - Started process (PID=126) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:51:05.910+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:51:05.912+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:51:05.911+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:51:06.437+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:51:06.422+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:51:06.440+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:51:06.477+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.572 seconds
[2023-10-15T20:51:36.733+0000] {processor.py:157} INFO - Started process (PID=137) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:51:36.734+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:51:36.736+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:51:36.735+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:51:37.258+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:51:37.245+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:51:37.260+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:51:37.291+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.562 seconds
[2023-10-15T20:52:07.686+0000] {processor.py:157} INFO - Started process (PID=148) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:52:07.687+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:52:07.690+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:52:07.689+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:52:09.504+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:52:09.458+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:52:09.510+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:52:09.562+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 1.884 seconds
[2023-10-15T20:52:40.849+0000] {processor.py:157} INFO - Started process (PID=159) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:52:40.855+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:52:40.862+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:52:40.861+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:52:42.687+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:52:42.660+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:52:42.690+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:52:42.747+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 1.904 seconds
[2023-10-15T20:53:13.012+0000] {processor.py:157} INFO - Started process (PID=170) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:53:13.013+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:53:13.017+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:53:13.016+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:53:13.590+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:53:13.575+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:53:13.593+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:53:13.625+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.618 seconds
[2023-10-15T20:53:43.877+0000] {processor.py:157} INFO - Started process (PID=181) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:53:43.878+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:53:43.880+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:53:43.879+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:53:44.426+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:53:44.410+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:53:44.429+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:53:44.466+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.596 seconds
[2023-10-15T20:54:14.811+0000] {processor.py:157} INFO - Started process (PID=192) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:54:14.813+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:54:14.819+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:54:14.818+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:54:15.471+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:54:15.442+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:54:15.474+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:54:15.508+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.703 seconds
[2023-10-15T20:54:45.803+0000] {processor.py:157} INFO - Started process (PID=203) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:54:45.804+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:54:45.805+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:54:45.805+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:54:46.459+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:54:46.422+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:54:46.463+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:54:46.509+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.713 seconds
[2023-10-15T20:55:16.842+0000] {processor.py:157} INFO - Started process (PID=214) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:55:16.844+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:55:16.846+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:55:16.846+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:55:17.968+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:55:17.943+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:55:17.973+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:55:18.312+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 1.479 seconds
[2023-10-15T20:55:49.198+0000] {processor.py:157} INFO - Started process (PID=225) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:55:49.204+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:55:49.206+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:55:49.206+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:55:49.789+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:55:49.772+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:55:49.792+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:55:49.823+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.635 seconds
[2023-10-15T20:56:20.085+0000] {processor.py:157} INFO - Started process (PID=236) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:56:20.086+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:56:20.088+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:56:20.087+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:56:20.615+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:56:20.592+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:56:20.618+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:56:20.653+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.573 seconds
[2023-10-15T20:56:50.917+0000] {processor.py:157} INFO - Started process (PID=247) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:56:50.918+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:56:50.920+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:56:50.919+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:56:51.640+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:56:51.623+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:56:51.643+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:56:51.675+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.764 seconds
[2023-10-15T20:57:22.230+0000] {processor.py:157} INFO - Started process (PID=258) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:57:22.241+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:57:22.245+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:57:22.244+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:57:29.090+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:57:28.364+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
  File "/opt/airflow/dags/etl.py", line 7, in <module>
    from load_To_Drive import subir_archivo
  File "/opt/airflow/dags/load_To_Drive.py", line 1, in <module>
    from pydrive2.auth import GoogleAuth
ModuleNotFoundError: No module named 'pydrive2'
[2023-10-15T20:57:29.095+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:57:30.482+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 8.261 seconds
[2023-10-15T20:58:01.190+0000] {processor.py:157} INFO - Started process (PID=272) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:58:01.193+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:58:01.195+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:58:01.194+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:58:02.023+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:58:02.012+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
ImportError: cannot import name 'load_drive' from 'etl' (/opt/airflow/dags/etl.py)
[2023-10-15T20:58:02.026+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:58:02.160+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.977 seconds
[2023-10-15T20:58:32.784+0000] {processor.py:157} INFO - Started process (PID=283) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:58:32.786+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:58:32.788+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:58:32.787+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:58:33.270+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:58:33.257+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
ImportError: cannot import name 'load_drive' from 'etl' (/opt/airflow/dags/etl.py)
[2023-10-15T20:58:33.272+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:58:33.319+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.540 seconds
[2023-10-15T20:59:03.570+0000] {processor.py:157} INFO - Started process (PID=294) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:59:03.571+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:59:03.572+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:59:03.572+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:59:04.210+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:59:04.202+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load, load_drive
ImportError: cannot import name 'load_drive' from 'etl' (/opt/airflow/dags/etl.py)
[2023-10-15T20:59:04.212+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:59:04.251+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.686 seconds
[2023-10-15T20:59:31.002+0000] {processor.py:157} INFO - Started process (PID=305) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T20:59:31.003+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T20:59:31.005+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:59:31.004+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:59:32.473+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T20:59:34.128+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:59:34.126+0000] {security.py:708} INFO - Not syncing DAG-level permissions for DAG 'DAG:etl_dag' as access control is unset.
[2023-10-15T20:59:34.132+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:59:34.131+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T20:59:34.328+0000] {logging_mixin.py:151} INFO - [2023-10-15T20:59:34.327+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T20:59:34.437+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 3.442 seconds
[2023-10-15T21:00:04.754+0000] {processor.py:157} INFO - Started process (PID=316) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:00:04.756+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:00:04.760+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:00:04.758+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:00:05.372+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:00:05.416+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:00:05.415+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:00:05.449+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:00:05.449+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:00:05.483+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.735 seconds
[2023-10-15T21:00:35.741+0000] {processor.py:157} INFO - Started process (PID=327) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:00:35.742+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:00:35.744+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:00:35.743+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:00:36.253+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:00:36.290+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:00:36.290+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:00:36.335+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:00:36.335+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:00:36.367+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.631 seconds
[2023-10-15T21:01:06.511+0000] {processor.py:157} INFO - Started process (PID=338) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:01:06.513+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:01:06.514+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:01:06.514+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:01:06.984+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:01:07.023+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:01:07.022+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:01:07.054+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:01:07.054+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:01:07.096+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.589 seconds
[2023-10-15T21:01:37.331+0000] {processor.py:157} INFO - Started process (PID=349) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:01:37.332+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:01:37.334+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:01:37.333+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:01:37.909+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:01:37.953+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:01:37.953+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:01:37.989+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:01:37.989+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:01:38.019+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.694 seconds
[2023-10-15T21:02:08.341+0000] {processor.py:157} INFO - Started process (PID=360) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:02:08.347+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:02:08.350+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:02:08.349+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:02:09.034+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:02:09.117+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:02:09.116+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:02:09.188+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:02:09.187+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:02:09.252+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.917 seconds
[2023-10-15T21:02:39.573+0000] {processor.py:157} INFO - Started process (PID=371) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:02:39.578+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:02:39.579+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:02:39.579+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:02:40.112+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:02:40.162+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:02:40.161+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:02:40.196+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:02:40.196+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:02:40.249+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.680 seconds
[2023-10-15T21:03:10.456+0000] {processor.py:157} INFO - Started process (PID=382) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:03:10.457+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:03:10.458+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:03:10.458+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:03:11.143+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:03:11.248+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:03:11.247+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:03:11.326+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:03:11.325+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:03:11.391+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.941 seconds
[2023-10-15T21:03:42.057+0000] {processor.py:157} INFO - Started process (PID=393) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:03:42.059+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:03:42.060+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:03:42.060+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:03:42.741+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:03:42.786+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:03:42.785+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:03:42.820+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:03:42.819+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:03:42.848+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.796 seconds
[2023-10-15T21:04:13.013+0000] {processor.py:157} INFO - Started process (PID=411) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:04:13.014+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:04:13.015+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:04:13.015+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:04:13.521+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:04:13.554+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:04:13.554+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:04:13.585+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:04:13.585+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:04:13.637+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.630 seconds
[2023-10-15T21:04:43.880+0000] {processor.py:157} INFO - Started process (PID=422) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:04:43.881+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:04:43.882+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:04:43.882+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:04:44.445+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:04:44.479+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:04:44.478+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:04:44.509+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:04:44.508+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:04:44.537+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.661 seconds
[2023-10-15T21:05:14.777+0000] {processor.py:157} INFO - Started process (PID=434) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:05:14.778+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:05:14.779+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:05:14.779+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:05:15.303+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:05:15.341+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:05:15.340+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:05:15.371+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:05:15.371+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:05:15.399+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.627 seconds
[2023-10-15T21:05:45.645+0000] {processor.py:157} INFO - Started process (PID=445) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:05:45.646+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:05:45.647+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:05:45.647+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:05:46.106+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:05:46.141+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:05:46.140+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:05:46.170+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:05:46.170+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:05:46.198+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.556 seconds
[2023-10-15T21:06:16.395+0000] {processor.py:157} INFO - Started process (PID=456) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:06:16.396+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:06:16.397+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:06:16.397+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:06:16.869+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:06:16.909+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:06:16.909+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:06:16.941+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:06:16.940+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:06:16.971+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.581 seconds
[2023-10-15T21:06:47.206+0000] {processor.py:157} INFO - Started process (PID=467) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:06:47.207+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:06:47.208+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:06:47.207+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:06:47.679+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:06:47.717+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:06:47.716+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:06:47.750+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:06:47.750+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:06:47.783+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.581 seconds
[2023-10-15T21:07:17.888+0000] {processor.py:157} INFO - Started process (PID=478) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:07:17.889+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:07:17.891+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:07:17.890+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:07:18.372+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:07:18.413+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:07:18.413+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:07:18.445+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:07:18.445+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:07:18.473+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.589 seconds
[2023-10-15T21:07:48.703+0000] {processor.py:157} INFO - Started process (PID=489) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:07:48.704+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:07:48.705+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:07:48.704+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:07:49.202+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:07:49.242+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:07:49.241+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:07:49.277+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:07:49.277+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:07:49.307+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.609 seconds
[2023-10-15T21:08:19.565+0000] {processor.py:157} INFO - Started process (PID=500) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:08:19.566+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:08:19.567+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:08:19.567+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:08:20.025+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:08:20.061+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:08:20.060+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:08:20.091+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:08:20.091+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:08:20.122+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.561 seconds
[2023-10-15T21:08:50.360+0000] {processor.py:157} INFO - Started process (PID=511) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:08:50.361+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:08:50.362+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:08:50.362+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:08:50.815+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:08:50.847+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:08:50.847+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:08:50.876+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:08:50.876+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:08:50.907+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.551 seconds
[2023-10-15T21:09:21.144+0000] {processor.py:157} INFO - Started process (PID=522) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:09:21.145+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:09:21.146+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:09:21.146+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:09:21.599+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:09:21.633+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:09:21.633+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:09:21.664+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:09:21.664+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:09:21.694+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.554 seconds
[2023-10-15T21:09:51.929+0000] {processor.py:157} INFO - Started process (PID=533) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:09:51.930+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:09:51.932+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:09:51.931+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:09:52.432+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:09:52.479+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:09:52.478+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:09:52.518+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:09:52.518+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:09:52.553+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.627 seconds
[2023-10-15T21:10:22.791+0000] {processor.py:157} INFO - Started process (PID=544) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:10:22.793+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:10:22.794+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:10:22.794+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:10:23.378+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:10:23.419+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:10:23.418+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:10:23.474+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:10:23.473+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:10:23.510+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.725 seconds
[2023-10-15T21:10:53.758+0000] {processor.py:157} INFO - Started process (PID=556) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:10:53.760+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:10:53.761+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:10:53.761+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:10:54.364+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:10:54.411+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:10:54.410+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:10:54.460+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:10:54.460+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:10:54.509+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.755 seconds
[2023-10-15T21:11:25.012+0000] {processor.py:157} INFO - Started process (PID=567) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T21:11:25.013+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T21:11:25.015+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:11:25.014+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:11:25.613+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T21:11:25.657+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:11:25.655+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T21:11:25.698+0000] {logging_mixin.py:151} INFO - [2023-10-15T21:11:25.697+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T21:11:25.734+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.728 seconds
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [2023-10-15T22:05:33.523+0000] {processor.py:157} INFO - Started process (PID=33) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:05:33.528+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:05:33.530+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:05:33.530+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:05:34.889+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:05:34.952+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:05:34.952+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:05:34.987+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:05:34.987+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:05:35.023+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 1.512 seconds
[2023-10-15T22:06:05.141+0000] {processor.py:157} INFO - Started process (PID=44) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:06:05.146+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:06:05.147+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:06:05.147+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:06:07.035+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:06:07.117+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:06:07.116+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:06:07.202+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:06:07.202+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:06:07.258+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 2.123 seconds
[2023-10-15T22:06:37.610+0000] {processor.py:157} INFO - Started process (PID=56) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:06:37.613+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:06:37.618+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:06:37.616+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:06:39.252+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:06:39.447+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:06:39.447+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:06:39.541+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:06:39.540+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:06:39.598+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 2.019 seconds
[2023-10-15T22:07:09.799+0000] {processor.py:157} INFO - Started process (PID=67) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:07:09.806+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:07:09.809+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:07:09.809+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:07:10.861+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:07:10.945+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:07:10.945+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:07:11.008+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:07:11.008+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:07:11.057+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 1.263 seconds
[2023-10-15T22:07:41.207+0000] {processor.py:157} INFO - Started process (PID=78) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:07:41.213+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:07:41.217+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:07:41.216+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:07:42.184+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:07:42.233+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:07:42.232+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:07:42.273+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:07:42.273+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:07:42.306+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 1.105 seconds
[2023-10-15T22:08:12.630+0000] {processor.py:157} INFO - Started process (PID=89) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:08:12.631+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:08:12.633+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:08:12.632+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:08:13.090+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:08:13.131+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:08:13.130+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:08:13.158+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:08:13.158+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:08:13.187+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.561 seconds
[2023-10-15T22:08:43.398+0000] {processor.py:157} INFO - Started process (PID=100) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:08:43.399+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:08:43.400+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:08:43.400+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:08:43.763+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:08:43.797+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:08:43.796+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:08:43.826+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:08:43.826+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:08:43.853+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.459 seconds
[2023-10-15T22:09:14.109+0000] {processor.py:157} INFO - Started process (PID=111) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:09:14.110+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:09:14.112+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:09:14.111+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:09:14.688+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:09:14.773+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:09:14.772+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:09:14.797+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:09:14.797+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:09:14.820+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.714 seconds
[2023-10-15T22:09:45.067+0000] {processor.py:157} INFO - Started process (PID=122) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:09:45.068+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:09:45.069+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:09:45.069+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:09:45.553+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:09:45.665+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:09:45.665+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:09:45.689+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:09:45.688+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:09:45.712+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.649 seconds
[2023-10-15T22:10:16.349+0000] {processor.py:157} INFO - Started process (PID=133) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:10:16.362+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:10:16.365+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:10:16.364+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:10:19.233+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:10:19.545+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:10:19.544+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:10:19.752+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:10:19.752+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:10:20.998+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 4.693 seconds
[2023-10-15T22:10:51.679+0000] {processor.py:157} INFO - Started process (PID=153) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:10:51.680+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:10:51.681+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:10:51.681+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:10:52.039+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:10:52.126+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:10:52.125+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:10:52.149+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:10:52.149+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:10:52.173+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.497 seconds
[2023-10-15T22:11:22.501+0000] {processor.py:157} INFO - Started process (PID=164) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:11:22.502+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:11:22.503+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:11:22.503+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:11:22.852+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:11:22.882+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:11:22.881+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:11:22.910+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:11:22.910+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:11:22.936+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.438 seconds
[2023-10-15T22:11:53.233+0000] {processor.py:157} INFO - Started process (PID=175) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:11:53.234+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:11:53.235+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:11:53.235+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:11:53.633+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:11:53.665+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:11:53.664+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:11:53.691+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:11:53.691+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:11:53.716+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.486 seconds
[2023-10-15T22:12:24.117+0000] {processor.py:157} INFO - Started process (PID=186) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:12:24.118+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:12:24.119+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:12:24.118+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:12:24.436+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:12:24.461+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:12:24.460+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:12:24.483+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:12:24.483+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:12:24.509+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.395 seconds
[2023-10-15T22:12:54.832+0000] {processor.py:157} INFO - Started process (PID=197) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:12:54.833+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:12:54.834+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:12:54.834+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:12:55.176+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:12:55.201+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:12:55.201+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:12:55.223+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:12:55.222+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:12:55.249+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.420 seconds
[2023-10-15T22:13:25.590+0000] {processor.py:157} INFO - Started process (PID=208) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:13:25.596+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:13:25.597+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:13:25.597+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:13:26.141+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:13:26.189+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:13:26.188+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:13:26.244+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:13:26.244+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:13:26.305+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.718 seconds
[2023-10-15T22:13:56.604+0000] {processor.py:157} INFO - Started process (PID=219) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:13:56.605+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:13:56.606+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:13:56.606+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:13:57.048+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:13:57.084+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:13:57.083+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:13:57.111+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:13:57.111+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:13:57.136+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.537 seconds
[2023-10-15T22:14:27.420+0000] {processor.py:157} INFO - Started process (PID=230) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:14:27.421+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:14:27.422+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:14:27.422+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:14:27.782+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:14:27.883+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:14:27.883+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:14:27.908+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:14:27.908+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:14:27.935+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.518 seconds
[2023-10-15T22:14:58.360+0000] {processor.py:157} INFO - Started process (PID=241) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:14:58.361+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:14:58.362+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:14:58.361+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:14:58.686+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:14:58.714+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:14:58.714+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:14:58.870+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:14:58.869+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:14:59.011+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.656 seconds
[2023-10-15T22:15:29.332+0000] {processor.py:157} INFO - Started process (PID=252) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:15:29.333+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:15:29.334+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:15:29.334+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:15:29.660+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:15:29.686+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:15:29.686+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:15:29.707+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:15:29.707+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:15:29.730+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.401 seconds
[2023-10-15T22:16:00.234+0000] {processor.py:157} INFO - Started process (PID=263) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:16:00.235+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:16:00.236+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:16:00.236+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:16:00.587+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:16:00.612+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:16:00.611+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:16:00.634+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:16:00.634+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:16:00.663+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.432 seconds
[2023-10-15T22:16:30.980+0000] {processor.py:157} INFO - Started process (PID=274) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:16:30.981+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:16:30.982+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:16:30.982+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:16:31.324+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:16:31.349+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:16:31.349+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:16:31.372+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:16:31.371+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:16:31.397+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.420 seconds
[2023-10-15T22:17:01.751+0000] {processor.py:157} INFO - Started process (PID=285) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:17:01.752+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:17:01.753+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:17:01.753+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:17:02.070+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:17:02.097+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:17:02.097+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:17:02.121+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:17:02.120+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:17:02.148+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.400 seconds
[2023-10-15T22:17:32.456+0000] {processor.py:157} INFO - Started process (PID=296) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:17:32.457+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:17:32.458+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:17:32.457+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:17:32.790+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:17:32.818+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:17:32.818+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:17:32.843+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:17:32.842+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:17:32.868+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.415 seconds
[2023-10-15T22:18:03.212+0000] {processor.py:157} INFO - Started process (PID=307) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:18:03.214+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:18:03.216+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:18:03.215+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:18:03.564+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:18:03.590+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:18:03.590+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:18:03.620+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:18:03.620+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:18:03.649+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.443 seconds
[2023-10-15T22:18:34.002+0000] {processor.py:157} INFO - Started process (PID=318) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:18:34.003+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:18:34.004+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:18:34.004+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:18:34.337+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:18:34.361+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:18:34.361+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:18:34.383+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:18:34.383+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:18:34.419+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.421 seconds
[2023-10-15T22:19:04.719+0000] {processor.py:157} INFO - Started process (PID=329) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:19:04.720+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:19:04.721+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:19:04.721+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:19:05.245+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:19:05.342+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:19:05.341+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:19:05.398+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:19:05.398+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:19:05.444+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.729 seconds
[2023-10-15T22:19:35.771+0000] {processor.py:157} INFO - Started process (PID=340) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:19:35.772+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:19:35.773+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:19:35.772+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:19:36.310+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:19:36.343+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:19:36.343+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:19:36.371+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:19:36.371+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:19:36.402+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.634 seconds
[2023-10-15T22:20:06.881+0000] {processor.py:157} INFO - Started process (PID=351) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:20:06.882+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:20:06.884+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:20:06.884+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:20:07.261+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:20:07.289+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:20:07.288+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:20:07.312+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:20:07.312+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:20:07.338+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.462 seconds
[2023-10-15T22:20:38.147+0000] {processor.py:157} INFO - Started process (PID=362) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:20:38.148+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:20:38.149+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:20:38.148+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:20:38.551+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:20:38.582+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:20:38.581+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:20:38.611+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:20:38.610+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:20:38.642+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.498 seconds
[2023-10-15T22:21:09.090+0000] {processor.py:157} INFO - Started process (PID=373) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:21:09.091+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:21:09.092+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:21:09.092+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:21:09.798+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:21:09.829+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:21:09.829+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:21:09.858+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:21:09.858+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:21:09.883+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.817 seconds
[2023-10-15T22:21:40.249+0000] {processor.py:157} INFO - Started process (PID=384) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:21:40.250+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:21:40.251+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:21:40.250+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:21:40.800+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:21:40.856+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:21:40.856+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:21:40.901+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:21:40.901+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:21:40.950+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.705 seconds
[2023-10-15T22:22:11.060+0000] {processor.py:157} INFO - Started process (PID=395) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:22:11.061+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:22:11.062+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:22:11.061+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:22:11.540+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:22:11.588+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:22:11.587+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:22:11.626+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:22:11.626+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:22:11.664+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.608 seconds
[2023-10-15T22:22:42.062+0000] {processor.py:157} INFO - Started process (PID=406) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:22:42.062+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:22:42.063+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:22:42.063+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:22:42.404+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:22:42.431+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:22:42.431+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:22:42.452+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:22:42.452+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:22:42.485+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.427 seconds
[2023-10-15T22:23:12.826+0000] {processor.py:157} INFO - Started process (PID=417) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:23:12.827+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:23:12.828+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:23:12.827+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:23:13.210+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:23:13.244+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:23:13.243+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:23:13.273+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:23:13.273+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:23:13.302+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.479 seconds
[2023-10-15T22:23:43.689+0000] {processor.py:157} INFO - Started process (PID=428) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:23:43.689+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:23:43.690+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:23:43.690+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:23:44.024+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:23:44.050+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:23:44.050+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:23:44.071+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:23:44.071+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:23:44.097+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.412 seconds
[2023-10-15T22:24:14.480+0000] {processor.py:157} INFO - Started process (PID=439) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:24:14.481+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:24:14.482+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:24:14.482+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:24:14.835+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:24:14.865+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:24:14.864+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:24:14.925+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:24:14.924+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:24:15.008+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.531 seconds
[2023-10-15T22:24:45.272+0000] {processor.py:157} INFO - Started process (PID=450) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:24:45.273+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:24:45.274+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:24:45.274+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:24:45.714+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:24:45.748+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:24:45.747+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:24:45.777+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:24:45.776+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:24:45.818+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.550 seconds
[2023-10-15T22:25:16.222+0000] {processor.py:157} INFO - Started process (PID=461) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:25:16.223+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:25:16.224+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:25:16.224+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:25:16.549+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:25:16.573+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:25:16.573+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:25:16.595+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:25:16.595+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:25:16.623+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.404 seconds
[2023-10-15T22:25:47.011+0000] {processor.py:157} INFO - Started process (PID=472) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:25:47.012+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:25:47.013+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:25:47.013+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:25:47.392+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:25:47.422+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:25:47.421+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:25:47.445+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:25:47.445+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:25:47.475+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.467 seconds
[2023-10-15T22:26:17.731+0000] {processor.py:157} INFO - Started process (PID=483) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:26:17.732+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:26:17.733+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:26:17.733+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:26:18.125+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:26:18.151+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:26:18.151+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:26:18.175+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:26:18.175+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:26:18.199+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.470 seconds
[2023-10-15T22:26:48.519+0000] {processor.py:157} INFO - Started process (PID=494) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:26:48.520+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:26:48.521+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:26:48.521+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:26:48.859+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:26:48.887+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:26:48.886+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:26:48.909+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:26:48.909+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:26:48.934+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.418 seconds
[2023-10-15T22:27:19.222+0000] {processor.py:157} INFO - Started process (PID=505) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:27:19.223+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:27:19.224+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:27:19.224+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:27:19.572+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:27:19.603+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:27:19.602+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:27:19.629+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:27:19.629+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:27:19.657+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.437 seconds
[2023-10-15T22:27:49.860+0000] {processor.py:157} INFO - Started process (PID=516) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:27:49.861+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:27:49.862+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:27:49.862+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:27:50.217+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:27:50.245+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:27:50.244+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:27:50.267+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:27:50.267+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:27:50.293+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.437 seconds
[2023-10-15T22:28:20.612+0000] {processor.py:157} INFO - Started process (PID=527) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:28:20.613+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:28:20.615+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:28:20.615+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:28:21.065+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:28:21.092+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:28:21.092+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:28:21.116+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:28:21.116+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:28:21.144+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.540 seconds
[2023-10-15T22:28:51.455+0000] {processor.py:157} INFO - Started process (PID=538) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:28:51.456+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:28:51.457+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:28:51.457+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:28:51.857+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:28:51.884+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:28:51.883+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:28:51.905+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:28:51.905+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:28:51.935+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.483 seconds
[2023-10-15T22:29:22.253+0000] {processor.py:157} INFO - Started process (PID=549) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:29:22.254+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:29:22.255+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:29:22.255+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:29:22.641+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:29:22.672+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:29:22.671+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:29:22.697+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:29:22.697+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:29:22.725+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.475 seconds
[2023-10-15T22:29:53.036+0000] {processor.py:157} INFO - Started process (PID=560) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:29:53.038+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:29:53.041+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:29:53.040+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:29:53.423+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:29:53.457+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:29:53.456+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:29:53.493+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:29:53.493+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:29:53.522+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.490 seconds
[2023-10-15T22:30:23.879+0000] {processor.py:157} INFO - Started process (PID=571) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:30:23.880+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:30:23.881+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:30:23.881+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:30:24.626+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:30:24.653+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:30:24.652+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:30:24.762+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:30:24.756+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:30:24.792+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.915 seconds
[2023-10-15T22:30:55.180+0000] {processor.py:157} INFO - Started process (PID=582) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:30:55.180+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:30:55.181+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:30:55.181+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:30:55.551+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:30:55.581+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:30:55.580+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:30:55.605+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:30:55.605+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:30:55.632+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.456 seconds
[2023-10-15T22:31:25.893+0000] {processor.py:157} INFO - Started process (PID=593) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:31:25.893+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:31:25.895+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:31:25.894+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:31:26.336+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:31:26.533+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:31:26.532+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:31:26.569+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:31:26.569+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:31:26.597+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.709 seconds
[2023-10-15T22:31:56.873+0000] {processor.py:157} INFO - Started process (PID=604) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:31:56.875+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:31:56.876+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:31:56.876+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:31:57.247+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:31:57.278+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:31:57.277+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:31:57.302+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:31:57.302+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:31:57.330+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.460 seconds
[2023-10-15T22:32:27.655+0000] {processor.py:157} INFO - Started process (PID=615) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:32:27.655+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:32:27.656+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:32:27.656+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:32:28.054+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:32:28.125+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:32:28.124+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:32:28.152+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:32:28.152+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:32:28.181+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.530 seconds
[2023-10-15T22:32:58.494+0000] {processor.py:157} INFO - Started process (PID=633) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:32:58.496+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:32:58.497+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:32:58.497+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:32:58.893+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:32:58.925+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:32:58.924+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:32:58.964+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:32:58.963+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:32:59.000+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.509 seconds
[2023-10-15T22:33:29.379+0000] {processor.py:157} INFO - Started process (PID=644) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:33:29.380+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:33:29.381+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:33:29.381+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:33:29.752+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:33:29.777+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:33:29.776+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:33:29.800+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:33:29.800+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:33:29.828+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.452 seconds
[2023-10-15T22:34:00.175+0000] {processor.py:157} INFO - Started process (PID=655) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:34:00.177+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:34:00.179+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:34:00.178+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:34:00.540+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:34:00.567+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:34:00.567+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:34:00.588+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:34:00.588+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:34:00.611+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.444 seconds
[2023-10-15T22:34:30.962+0000] {processor.py:157} INFO - Started process (PID=666) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:34:30.962+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:34:30.964+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:34:30.964+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:34:31.347+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:34:31.373+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:34:31.373+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:34:31.398+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:34:31.397+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:34:31.431+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.473 seconds
[2023-10-15T22:35:01.756+0000] {processor.py:157} INFO - Started process (PID=677) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:35:01.756+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:35:01.757+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:35:01.757+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:35:02.124+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:35:02.152+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:35:02.152+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:35:02.179+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:35:02.179+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:35:02.210+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.457 seconds
[2023-10-15T22:35:32.541+0000] {processor.py:157} INFO - Started process (PID=688) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:35:32.542+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:35:32.543+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:35:32.543+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:35:32.899+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:35:32.925+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:35:32.925+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:35:32.948+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:35:32.947+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:35:32.982+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.444 seconds
[2023-10-15T22:36:03.328+0000] {processor.py:157} INFO - Started process (PID=699) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:36:03.330+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:36:03.331+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:36:03.331+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:36:03.775+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:36:03.837+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:36:03.836+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:36:03.861+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:36:03.861+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:36:03.886+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.562 seconds
[2023-10-15T22:36:34.167+0000] {processor.py:157} INFO - Started process (PID=710) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:36:34.168+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:36:34.169+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:36:34.169+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:36:34.510+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:36:34.535+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:36:34.535+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:36:34.556+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:36:34.556+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:36:34.582+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.418 seconds
[2023-10-15T22:37:04.931+0000] {processor.py:157} INFO - Started process (PID=721) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:37:04.932+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:37:04.933+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:37:04.933+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:37:05.264+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:37:05.290+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:37:05.289+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:37:05.311+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:37:05.311+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:37:05.340+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.411 seconds
[2023-10-15T22:37:35.749+0000] {processor.py:157} INFO - Started process (PID=732) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:37:35.750+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:37:35.751+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:37:35.751+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:37:36.079+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:37:36.103+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:37:36.102+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:37:36.128+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:37:36.128+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:37:36.160+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.415 seconds
[2023-10-15T22:38:06.530+0000] {processor.py:157} INFO - Started process (PID=743) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:38:06.531+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:38:06.533+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:38:06.533+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:38:07.039+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:38:07.082+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:38:07.081+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:38:07.120+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:38:07.120+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:38:07.159+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.633 seconds
[2023-10-15T22:38:37.408+0000] {processor.py:157} INFO - Started process (PID=754) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:38:37.409+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:38:37.411+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:38:37.410+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:38:37.876+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:38:37.910+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:38:37.910+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:38:37.940+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:38:37.940+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:38:37.972+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.570 seconds
[2023-10-15T22:39:08.260+0000] {processor.py:157} INFO - Started process (PID=765) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:39:08.260+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:39:08.261+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:39:08.261+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:39:08.597+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:39:08.622+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:39:08.621+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:39:08.642+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:39:08.642+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:39:08.677+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.420 seconds
[2023-10-15T22:39:38.986+0000] {processor.py:157} INFO - Started process (PID=776) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:39:38.987+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:39:38.989+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:39:38.988+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:39:39.357+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:39:39.385+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:39:39.384+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:39:39.410+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:39:39.410+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:39:39.438+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.456 seconds
[2023-10-15T22:40:09.762+0000] {processor.py:157} INFO - Started process (PID=787) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:40:09.763+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:40:09.764+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:40:09.764+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:40:10.294+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:40:10.333+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:40:10.333+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:40:10.358+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:40:10.358+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:40:10.383+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.625 seconds
[2023-10-15T22:40:40.713+0000] {processor.py:157} INFO - Started process (PID=798) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:40:40.714+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:40:40.715+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:40:40.715+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:40:41.052+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:40:41.079+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:40:41.079+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:40:41.105+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:40:41.104+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:40:41.135+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.424 seconds
[2023-10-15T22:41:11.428+0000] {processor.py:157} INFO - Started process (PID=809) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:41:11.429+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:41:11.430+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:41:11.429+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:41:11.756+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:41:11.781+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:41:11.781+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:41:11.802+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:41:11.802+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:41:11.831+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.406 seconds
[2023-10-15T22:41:42.157+0000] {processor.py:157} INFO - Started process (PID=820) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:41:42.157+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:41:42.158+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:41:42.158+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:41:42.489+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:41:42.513+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:41:42.513+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:41:42.534+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:41:42.534+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:41:42.561+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.407 seconds
[2023-10-15T22:42:12.905+0000] {processor.py:157} INFO - Started process (PID=831) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:42:12.906+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:42:12.907+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:42:12.906+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:42:13.324+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:42:13.367+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:42:13.367+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:42:13.392+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:42:13.392+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:42:13.423+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.521 seconds
[2023-10-15T22:42:43.793+0000] {processor.py:157} INFO - Started process (PID=842) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:42:43.794+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:42:43.795+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:42:43.795+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:42:44.207+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:42:44.236+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:42:44.235+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:42:44.258+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:42:44.258+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:42:44.287+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.497 seconds
[2023-10-15T22:43:14.584+0000] {processor.py:157} INFO - Started process (PID=853) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:43:14.585+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:43:14.587+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:43:14.586+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:43:15.591+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:43:15.665+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:43:15.664+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:43:15.723+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:43:15.723+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:43:15.811+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 1.231 seconds
[2023-10-15T22:43:46.130+0000] {processor.py:157} INFO - Started process (PID=864) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:43:46.132+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:43:46.133+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:43:46.133+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:43:46.564+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:43:46.603+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:43:46.602+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:43:46.640+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:43:46.640+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:43:46.675+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.547 seconds
[2023-10-15T22:44:17.033+0000] {processor.py:157} INFO - Started process (PID=875) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:44:17.034+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:44:17.035+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:44:17.035+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:44:17.387+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:44:17.412+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:44:17.412+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:44:17.433+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:44:17.433+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:44:17.458+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.429 seconds
[2023-10-15T22:44:47.769+0000] {processor.py:157} INFO - Started process (PID=886) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:44:47.771+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:44:47.772+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:44:47.772+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:44:48.186+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:44:48.214+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:44:48.214+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:44:48.242+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:44:48.242+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:44:48.270+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.505 seconds
[2023-10-15T22:45:18.617+0000] {processor.py:157} INFO - Started process (PID=897) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:45:18.618+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:45:18.619+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:45:18.619+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:45:18.957+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:45:18.984+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:45:18.984+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:45:19.010+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:45:19.010+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:45:19.036+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.422 seconds
[2023-10-15T22:45:49.336+0000] {processor.py:157} INFO - Started process (PID=908) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:45:49.337+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:45:49.338+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:45:49.338+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:45:49.786+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:45:49.817+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:45:49.817+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:45:49.846+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:45:49.845+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:45:49.881+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.548 seconds
[2023-10-15T22:46:20.062+0000] {processor.py:157} INFO - Started process (PID=919) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:46:20.064+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:46:20.068+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:46:20.066+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:46:20.517+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:46:20.571+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:46:20.570+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:46:20.625+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:46:20.624+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:46:20.658+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.602 seconds
[2023-10-15T22:46:50.801+0000] {processor.py:157} INFO - Started process (PID=930) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:46:50.802+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:46:50.804+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:46:50.803+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:46:51.223+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:46:51.257+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:46:51.256+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:46:51.284+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:46:51.284+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:46:51.316+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.520 seconds
[2023-10-15T22:47:21.552+0000] {processor.py:157} INFO - Started process (PID=941) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:47:21.553+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:47:21.554+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:47:21.554+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:47:21.989+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:47:22.029+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:47:22.028+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:47:22.072+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:47:22.072+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:47:22.108+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.560 seconds
[2023-10-15T22:47:52.412+0000] {processor.py:157} INFO - Started process (PID=952) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:47:52.412+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:47:52.414+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:47:52.414+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:47:52.790+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:47:52.819+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:47:52.819+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:47:52.843+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:47:52.843+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:47:52.870+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.461 seconds
[2023-10-15T22:48:23.206+0000] {processor.py:157} INFO - Started process (PID=963) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:48:23.207+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:48:23.208+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:48:23.207+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:48:23.575+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:48:23.604+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:48:23.603+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:48:23.637+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:48:23.637+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:48:23.664+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.460 seconds
[2023-10-15T22:48:54.024+0000] {processor.py:157} INFO - Started process (PID=974) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:48:54.025+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:48:54.027+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:48:54.027+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:48:54.447+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:48:54.476+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:48:54.476+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:48:54.500+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:48:54.500+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:48:54.558+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.542 seconds
[2023-10-15T22:49:24.842+0000] {processor.py:157} INFO - Started process (PID=985) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:49:24.843+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:49:24.844+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:49:24.844+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:49:25.232+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:49:25.270+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:49:25.269+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:49:25.297+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:49:25.297+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:49:25.332+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.492 seconds
[2023-10-15T22:49:55.642+0000] {processor.py:157} INFO - Started process (PID=996) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:49:55.643+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:49:55.644+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:49:55.644+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:49:55.990+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:49:56.014+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:49:56.013+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:49:56.036+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:49:56.036+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:49:56.063+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.424 seconds
[2023-10-15T22:50:26.361+0000] {processor.py:157} INFO - Started process (PID=1007) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:50:26.363+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:50:26.364+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:50:26.364+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:50:26.741+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:50:26.774+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:50:26.773+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:50:26.802+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:50:26.802+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:50:26.832+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.475 seconds
[2023-10-15T22:50:57.382+0000] {processor.py:157} INFO - Started process (PID=1018) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:50:57.383+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:50:57.384+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:50:57.383+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:50:57.732+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:50:57.759+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:50:57.759+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:50:57.782+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:50:57.781+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:50:57.809+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.430 seconds
[2023-10-15T22:51:28.148+0000] {processor.py:157} INFO - Started process (PID=1029) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:51:28.149+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:51:28.150+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:51:28.150+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:51:28.580+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:51:28.606+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:51:28.605+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:51:28.630+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:51:28.629+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:51:28.655+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.512 seconds
[2023-10-15T22:51:59.003+0000] {processor.py:157} INFO - Started process (PID=1040) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:51:59.004+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:51:59.005+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:51:59.005+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:51:59.731+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:51:59.779+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:51:59.778+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:51:59.839+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:51:59.839+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:51:59.870+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.871 seconds
[2023-10-15T22:52:30.229+0000] {processor.py:157} INFO - Started process (PID=1051) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:52:30.230+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:52:30.232+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:52:30.232+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:52:30.750+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:52:30.784+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:52:30.784+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:52:30.814+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:52:30.814+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:52:30.840+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.614 seconds
[2023-10-15T22:53:01.117+0000] {processor.py:157} INFO - Started process (PID=1062) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:53:01.119+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:53:01.120+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:53:01.120+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:53:01.605+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:53:01.651+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:53:01.649+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:53:01.694+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:53:01.694+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:53:01.726+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.614 seconds
[2023-10-15T22:53:32.061+0000] {processor.py:157} INFO - Started process (PID=1073) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:53:32.062+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:53:32.063+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:53:32.062+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:53:32.391+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:53:32.417+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:53:32.416+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:53:32.439+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:53:32.439+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:53:32.471+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.414 seconds
[2023-10-15T22:54:02.719+0000] {processor.py:157} INFO - Started process (PID=1084) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:54:02.720+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:54:02.721+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:54:02.721+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:54:03.129+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:54:03.158+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:54:03.157+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:54:03.182+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:54:03.182+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:54:03.207+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.492 seconds
[2023-10-15T22:54:33.453+0000] {processor.py:157} INFO - Started process (PID=1095) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:54:33.454+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:54:33.455+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:54:33.454+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:54:33.963+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:54:33.998+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:54:33.997+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:54:34.024+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:54:34.024+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:54:34.079+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.630 seconds
[2023-10-15T22:55:04.195+0000] {processor.py:157} INFO - Started process (PID=1106) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:55:04.196+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:55:04.197+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:55:04.197+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:55:04.664+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:55:04.700+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:55:04.700+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:55:04.730+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:55:04.729+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:55:04.764+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.574 seconds
[2023-10-15T22:55:35.262+0000] {processor.py:157} INFO - Started process (PID=1124) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:55:35.263+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:55:35.264+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:55:35.264+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:55:35.642+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:55:35.671+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:55:35.670+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:55:35.704+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:55:35.704+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:55:35.745+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.487 seconds
[2023-10-15T22:56:05.819+0000] {processor.py:157} INFO - Started process (PID=1135) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:56:05.820+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:56:05.821+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:56:05.820+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:56:06.246+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:56:06.274+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:56:06.273+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:56:06.299+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:56:06.299+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:56:06.324+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.509 seconds
[2023-10-15T22:56:36.635+0000] {processor.py:157} INFO - Started process (PID=1146) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:56:36.636+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:56:36.638+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:56:36.637+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:56:37.062+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:56:37.095+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:56:37.094+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:56:37.131+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:56:37.131+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:56:37.162+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.530 seconds
[2023-10-15T22:57:07.533+0000] {processor.py:157} INFO - Started process (PID=1157) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:57:07.534+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:57:07.535+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:57:07.535+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:57:07.891+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:57:07.917+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:57:07.917+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:57:07.942+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:57:07.942+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:57:07.971+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.441 seconds
[2023-10-15T22:57:38.331+0000] {processor.py:157} INFO - Started process (PID=1168) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:57:38.332+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:57:38.333+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:57:38.333+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:57:38.715+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:57:38.742+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:57:38.742+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:57:38.765+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:57:38.765+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:57:38.792+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.464 seconds
[2023-10-15T22:58:09.149+0000] {processor.py:157} INFO - Started process (PID=1179) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:58:09.151+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:58:09.153+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:58:09.152+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:58:09.559+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:58:09.592+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:58:09.591+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:58:09.623+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:58:09.623+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:58:09.654+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.508 seconds
[2023-10-15T22:58:40.029+0000] {processor.py:157} INFO - Started process (PID=1190) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:58:40.030+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:58:40.031+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:58:40.031+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:58:40.477+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:58:40.517+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:58:40.516+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:58:40.557+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:58:40.556+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:58:40.592+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.567 seconds
[2023-10-15T22:59:10.926+0000] {processor.py:157} INFO - Started process (PID=1201) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:59:10.927+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:59:10.928+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:59:10.928+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:59:11.442+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:59:11.486+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:59:11.485+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:59:11.520+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:59:11.519+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:59:11.556+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.633 seconds
[2023-10-15T22:59:41.658+0000] {processor.py:157} INFO - Started process (PID=1212) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T22:59:41.659+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T22:59:41.660+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:59:41.660+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:59:42.043+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T22:59:42.073+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:59:42.073+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T22:59:42.102+0000] {logging_mixin.py:151} INFO - [2023-10-15T22:59:42.102+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T22:59:42.129+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.474 seconds
[2023-10-15T23:00:12.462+0000] {processor.py:157} INFO - Started process (PID=1223) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:00:12.463+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:00:12.464+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:00:12.464+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:00:13.004+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:00:13.031+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:00:13.031+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:00:13.055+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:00:13.055+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:00:13.084+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.626 seconds
[2023-10-15T23:00:43.769+0000] {processor.py:157} INFO - Started process (PID=1235) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:00:43.770+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:00:43.771+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:00:43.771+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:00:44.218+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:00:44.253+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:00:44.252+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:00:44.283+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:00:44.283+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:00:44.319+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.553 seconds
[2023-10-15T23:01:14.595+0000] {processor.py:157} INFO - Started process (PID=1246) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:01:14.596+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:01:14.598+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:01:14.597+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:01:15.064+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:01:15.093+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:01:15.092+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:01:15.118+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:01:15.118+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:01:15.148+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.556 seconds
[2023-10-15T23:01:45.453+0000] {processor.py:157} INFO - Started process (PID=1257) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:01:45.454+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:01:45.455+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:01:45.455+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:01:45.924+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:01:45.957+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:01:45.957+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:01:45.987+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:01:45.987+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:01:46.020+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.571 seconds
[2023-10-15T23:02:16.191+0000] {processor.py:157} INFO - Started process (PID=1268) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:02:16.198+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:02:16.201+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:02:16.200+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:02:17.017+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:02:17.074+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:02:17.073+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:02:17.119+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:02:17.119+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:02:17.155+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.971 seconds
[2023-10-15T23:02:47.509+0000] {processor.py:157} INFO - Started process (PID=1279) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:02:47.510+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:02:47.511+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:02:47.510+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:02:47.934+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:02:47.962+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:02:47.962+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:02:47.986+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:02:47.986+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:02:48.012+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.507 seconds
[2023-10-15T23:03:18.330+0000] {processor.py:157} INFO - Started process (PID=1290) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:03:18.331+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:03:18.333+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:03:18.332+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:03:18.679+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:03:18.705+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:03:18.704+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:03:18.727+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:03:18.726+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:03:18.759+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.432 seconds
[2023-10-15T23:03:49.135+0000] {processor.py:157} INFO - Started process (PID=1301) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:03:49.136+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:03:49.137+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:03:49.137+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:03:49.493+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:03:49.520+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:03:49.519+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:03:49.544+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:03:49.544+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:03:49.574+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.443 seconds
[2023-10-15T23:04:19.921+0000] {processor.py:157} INFO - Started process (PID=1312) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:04:19.923+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:04:19.924+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:04:19.923+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:04:20.325+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:04:20.356+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:04:20.355+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:04:20.382+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:04:20.382+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:04:20.423+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.506 seconds
[2023-10-15T23:04:50.669+0000] {processor.py:157} INFO - Started process (PID=1323) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:04:50.671+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:04:50.673+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:04:50.672+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:04:51.015+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:04:51.042+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:04:51.042+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:04:51.064+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:04:51.064+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:04:51.092+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.426 seconds
[2023-10-15T23:05:21.410+0000] {processor.py:157} INFO - Started process (PID=1334) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:05:21.410+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:05:21.411+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:05:21.411+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:05:21.771+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:05:21.797+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:05:21.797+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:05:21.821+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:05:21.820+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:05:21.850+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.444 seconds
[2023-10-15T23:05:52.179+0000] {processor.py:157} INFO - Started process (PID=1345) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:05:52.180+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:05:52.181+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:05:52.180+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:05:52.699+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:05:52.731+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:05:52.731+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:05:52.761+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:05:52.761+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:05:52.796+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.623 seconds
[2023-10-15T23:06:23.102+0000] {processor.py:157} INFO - Started process (PID=1356) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:06:23.104+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:06:23.105+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:06:23.105+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:06:23.477+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:06:23.502+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:06:23.502+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:06:23.526+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:06:23.526+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:06:23.551+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.453 seconds
[2023-10-15T23:06:53.824+0000] {processor.py:157} INFO - Started process (PID=1367) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:06:53.824+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:06:53.826+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:06:53.825+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:06:54.279+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:06:54.319+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:06:54.318+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:06:54.346+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:06:54.345+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:06:54.373+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.552 seconds
[2023-10-15T23:07:24.554+0000] {processor.py:157} INFO - Started process (PID=1378) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:07:24.555+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:07:24.556+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:07:24.556+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:07:24.922+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:07:24.951+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:07:24.950+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:07:24.980+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:07:24.980+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:07:25.013+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.463 seconds
[2023-10-15T23:07:55.315+0000] {processor.py:157} INFO - Started process (PID=1389) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:07:55.316+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:07:55.318+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:07:55.317+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:07:55.793+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:07:55.825+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:07:55.825+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:07:55.859+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:07:55.858+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:07:55.903+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.592 seconds
[2023-10-15T23:08:26.032+0000] {processor.py:157} INFO - Started process (PID=1400) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:08:26.033+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:08:26.034+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:08:26.034+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:08:26.380+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:08:26.411+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:08:26.410+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:08:26.435+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:08:26.435+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:08:26.465+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.436 seconds
[2023-10-15T23:08:56.806+0000] {processor.py:157} INFO - Started process (PID=1411) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:08:56.807+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:08:56.808+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:08:56.808+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:08:57.146+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:08:57.173+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:08:57.173+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:08:57.195+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:08:57.195+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:08:57.221+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.418 seconds
[2023-10-15T23:09:27.552+0000] {processor.py:157} INFO - Started process (PID=1422) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:09:27.553+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:09:27.554+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:09:27.553+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:09:27.904+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:09:27.932+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:09:27.931+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:09:27.955+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:09:27.955+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:09:27.981+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.432 seconds
[2023-10-15T23:09:58.324+0000] {processor.py:157} INFO - Started process (PID=1433) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:09:58.325+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:09:58.327+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:09:58.326+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:09:58.710+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:09:58.742+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:09:58.741+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:09:58.777+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:09:58.776+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:09:58.804+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.484 seconds
[2023-10-15T23:10:29.136+0000] {processor.py:157} INFO - Started process (PID=1444) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:10:29.137+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:10:29.138+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:10:29.138+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:10:29.524+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:10:29.550+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:10:29.550+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:10:29.573+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:10:29.573+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:10:29.600+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.467 seconds
[2023-10-15T23:10:59.681+0000] {processor.py:157} INFO - Started process (PID=1455) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:10:59.682+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:10:59.683+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:10:59.683+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:11:00.045+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:11:00.073+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:11:00.072+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:11:00.097+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:11:00.097+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:11:00.121+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.443 seconds
[2023-10-15T23:11:30.472+0000] {processor.py:157} INFO - Started process (PID=1466) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:11:30.473+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:11:30.474+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:11:30.473+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:11:30.833+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:11:30.860+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:11:30.860+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:11:30.884+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:11:30.883+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:11:30.910+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.441 seconds
[2023-10-15T23:12:01.212+0000] {processor.py:157} INFO - Started process (PID=1477) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:12:01.212+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:12:01.213+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:12:01.213+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:12:01.584+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:12:01.624+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:12:01.622+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:12:01.655+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:12:01.654+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:12:01.682+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.473 seconds
[2023-10-15T23:12:31.942+0000] {processor.py:157} INFO - Started process (PID=1488) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:12:31.943+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:12:31.944+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:12:31.944+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:12:32.300+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:12:32.328+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:12:32.328+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:12:32.354+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:12:32.354+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:12:32.377+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.438 seconds
[2023-10-15T23:13:02.584+0000] {processor.py:157} INFO - Started process (PID=1499) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:13:02.585+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:13:02.587+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:13:02.586+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:13:02.936+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:13:02.962+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:13:02.962+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:13:02.983+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:13:02.983+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:13:03.013+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.433 seconds
[2023-10-15T23:13:33.380+0000] {processor.py:157} INFO - Started process (PID=1510) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:13:33.381+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:13:33.382+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:13:33.382+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:13:33.741+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:13:33.767+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:13:33.766+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:13:33.789+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:13:33.789+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:13:33.815+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.437 seconds
[2023-10-15T23:14:04.077+0000] {processor.py:157} INFO - Started process (PID=1521) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:14:04.078+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:14:04.080+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:14:04.079+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:14:04.426+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:14:04.459+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:14:04.458+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:14:04.487+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:14:04.487+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:14:05.980+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 1.905 seconds
[2023-10-15T23:14:36.316+0000] {processor.py:157} INFO - Started process (PID=1532) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:14:36.317+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:14:36.318+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:14:36.318+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:14:36.659+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:14:36.685+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:14:36.684+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:14:36.709+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:14:36.709+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:14:36.736+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.423 seconds
[2023-10-15T23:15:07.037+0000] {processor.py:157} INFO - Started process (PID=1543) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:15:07.038+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:15:07.039+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:15:07.038+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:15:07.384+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:15:07.410+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:15:07.409+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:15:07.433+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:15:07.432+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:15:07.460+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.426 seconds
[2023-10-15T23:15:37.983+0000] {processor.py:157} INFO - Started process (PID=1555) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:15:37.984+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:15:37.985+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:15:37.985+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:15:38.329+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:15:38.354+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:15:38.353+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:15:38.377+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:15:38.376+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-14T00:00:00+00:00, run_after=2023-10-15T00:00:00+00:00
[2023-10-15T23:15:38.402+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.422 seconds
[2023-10-15T23:30:00.246+0000] {processor.py:157} INFO - Started process (PID=1844) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:30:00.248+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:30:00.251+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:30:00.250+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:30:00.868+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:30:01.224+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:30:01.224+0000] {security.py:708} INFO - Not syncing DAG-level permissions for DAG 'DAG:etl_dag' as access control is unset.
[2023-10-15T23:30:01.227+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:30:01.226+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:30:01.245+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:30:01.244+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:30:01.287+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 1.045 seconds
[2023-10-15T23:30:31.709+0000] {processor.py:157} INFO - Started process (PID=1855) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:30:31.710+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:30:31.713+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:30:31.712+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:30:32.077+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:30:32.104+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:30:32.103+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:30:32.127+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:30:32.126+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:30:32.151+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.454 seconds
[2023-10-15T23:31:02.471+0000] {processor.py:157} INFO - Started process (PID=1866) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:31:02.472+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:31:02.475+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:31:02.474+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:31:02.836+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:31:02.864+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:31:02.864+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:31:02.884+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:31:02.884+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:31:02.908+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.442 seconds
[2023-10-15T23:31:33.524+0000] {processor.py:157} INFO - Started process (PID=1877) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:31:33.525+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:31:33.526+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:31:33.526+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:31:33.893+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:31:33.924+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:31:33.924+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:31:33.948+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:31:33.948+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:31:33.971+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.451 seconds
[2023-10-15T23:32:04.375+0000] {processor.py:157} INFO - Started process (PID=1892) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:32:04.376+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:32:04.378+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:32:04.378+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:32:04.822+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:32:04.851+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:32:04.850+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:32:04.870+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:32:04.870+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:32:04.891+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.520 seconds
[2023-10-15T23:32:35.355+0000] {processor.py:157} INFO - Started process (PID=1903) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:32:35.357+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:32:35.359+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:32:35.359+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:32:35.807+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:32:35.839+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:32:35.838+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:32:35.864+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:32:35.864+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:32:35.897+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.547 seconds
[2023-10-15T23:33:06.392+0000] {processor.py:157} INFO - Started process (PID=1914) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:33:06.394+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:33:06.396+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:33:06.396+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:33:06.858+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:33:06.887+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:33:06.886+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:33:06.909+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:33:06.909+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:33:06.934+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.545 seconds
[2023-10-15T23:33:37.489+0000] {processor.py:157} INFO - Started process (PID=1925) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:33:37.491+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:33:37.493+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:33:37.493+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:33:37.898+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:33:37.927+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:33:37.927+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:33:37.945+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:33:37.945+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:33:37.974+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.489 seconds
[2023-10-15T23:34:08.459+0000] {processor.py:157} INFO - Started process (PID=1940) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:34:08.461+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:34:08.463+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:34:08.462+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:34:08.913+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:34:08.941+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:34:08.941+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:34:08.960+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:34:08.960+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:34:08.982+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.528 seconds
[2023-10-15T23:34:39.498+0000] {processor.py:157} INFO - Started process (PID=1951) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:34:39.500+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:34:39.502+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:34:39.501+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:34:39.935+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:34:39.975+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:34:39.975+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:34:40.000+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:34:40.000+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:34:40.026+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.532 seconds
[2023-10-15T23:35:10.366+0000] {processor.py:157} INFO - Started process (PID=1962) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:35:10.368+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:35:10.371+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:35:10.370+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:35:10.794+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:35:10.823+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:35:10.823+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:35:10.846+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:35:10.846+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:35:10.876+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.514 seconds
[2023-10-15T23:35:41.371+0000] {processor.py:157} INFO - Started process (PID=1977) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:35:41.372+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:35:41.374+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:35:41.374+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:35:41.764+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:35:41.794+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:35:41.793+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:35:41.826+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:35:41.826+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:35:41.851+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.483 seconds
[2023-10-15T23:36:12.289+0000] {processor.py:157} INFO - Started process (PID=1988) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:36:12.291+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:36:12.293+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:36:12.293+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:36:12.684+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:36:12.714+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:36:12.714+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:36:12.737+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:36:12.737+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:36:12.764+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.479 seconds
[2023-10-15T23:36:43.232+0000] {processor.py:157} INFO - Started process (PID=2004) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:36:43.233+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:36:43.236+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:36:43.235+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:36:43.646+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:36:43.676+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:36:43.675+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:36:43.703+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:36:43.703+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:36:43.734+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.507 seconds
[2023-10-15T23:37:14.148+0000] {processor.py:157} INFO - Started process (PID=2015) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:37:14.150+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:37:14.152+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:37:14.152+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:37:14.583+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:37:14.612+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:37:14.611+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:37:14.636+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:37:14.636+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:37:14.665+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.521 seconds
[2023-10-15T23:37:45.136+0000] {processor.py:157} INFO - Started process (PID=2026) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:37:45.138+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:37:45.141+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:37:45.140+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:37:45.539+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:37:45.570+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:37:45.569+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:37:45.592+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:37:45.592+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:37:45.617+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.484 seconds
[2023-10-15T23:38:16.104+0000] {processor.py:157} INFO - Started process (PID=2037) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:38:16.105+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:38:16.107+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:38:16.107+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:38:16.515+0000] {processor.py:839} INFO - DAG(s) dict_keys(['etl_dag']) retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:38:16.538+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:38:16.538+0000] {dag.py:2915} INFO - Sync 1 DAGs
[2023-10-15T23:38:16.555+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:38:16.555+0000] {dag.py:3696} INFO - Setting next_dagrun for etl_dag to 2023-10-15T00:00:00+00:00, run_after=2023-10-16T00:00:00+00:00
[2023-10-15T23:38:16.576+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.476 seconds
[2023-10-15T23:38:46.702+0000] {processor.py:157} INFO - Started process (PID=2049) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:38:46.703+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:38:46.705+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:38:46.705+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:38:47.125+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:38:47.100+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:38:47.127+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:38:47.150+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.451 seconds
[2023-10-15T23:39:17.262+0000] {processor.py:157} INFO - Started process (PID=2060) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:39:17.264+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:39:17.266+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:39:17.266+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:39:17.720+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:39:17.689+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:39:17.722+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:39:17.750+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.492 seconds
[2023-10-15T23:39:47.867+0000] {processor.py:157} INFO - Started process (PID=2071) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:39:47.869+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:39:47.872+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:39:47.871+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:39:48.320+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:39:48.289+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:39:48.321+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:39:48.345+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.482 seconds
[2023-10-15T23:40:18.591+0000] {processor.py:157} INFO - Started process (PID=2082) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:40:18.593+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:40:18.595+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:40:18.595+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:40:19.030+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:40:18.996+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:40:19.032+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:40:19.059+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.473 seconds
[2023-10-15T23:40:49.579+0000] {processor.py:157} INFO - Started process (PID=2093) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:40:49.581+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:40:49.583+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:40:49.583+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:40:50.034+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:40:50.004+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:40:50.035+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:40:50.063+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.488 seconds
[2023-10-15T23:41:20.633+0000] {processor.py:157} INFO - Started process (PID=2104) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:41:20.635+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:41:20.637+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:41:20.637+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:41:21.099+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:41:21.064+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:41:21.101+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:41:21.126+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.496 seconds
[2023-10-15T23:41:51.716+0000] {processor.py:157} INFO - Started process (PID=2115) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:41:51.718+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:41:51.720+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:41:51.720+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:41:52.166+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:41:52.135+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:41:52.167+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:41:52.188+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.476 seconds
[2023-10-15T23:42:22.786+0000] {processor.py:157} INFO - Started process (PID=2126) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:42:22.788+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:42:22.790+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:42:22.790+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:42:23.248+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:42:23.216+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:42:23.250+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:42:23.272+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.490 seconds
[2023-10-15T23:42:53.882+0000] {processor.py:157} INFO - Started process (PID=2137) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:42:53.884+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:42:53.885+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:42:53.885+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:42:54.341+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:42:54.303+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:42:54.343+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:42:54.370+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.491 seconds
[2023-10-15T23:43:25.010+0000] {processor.py:157} INFO - Started process (PID=2155) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:43:25.012+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:43:25.014+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:43:25.014+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:43:25.433+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:43:25.400+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:43:25.434+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:43:25.459+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.455 seconds
[2023-10-15T23:43:56.101+0000] {processor.py:157} INFO - Started process (PID=2166) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:43:56.102+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:43:56.104+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:43:56.104+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:43:56.523+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:43:56.491+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:43:56.525+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:43:56.551+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.453 seconds
[2023-10-15T23:44:27.154+0000] {processor.py:157} INFO - Started process (PID=2177) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:44:27.156+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:44:27.158+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:44:27.158+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:44:27.608+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:44:27.570+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:44:27.610+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:44:27.637+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.486 seconds
[2023-10-15T23:44:58.204+0000] {processor.py:157} INFO - Started process (PID=2188) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:44:58.206+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:44:58.208+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:44:58.207+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:44:58.681+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:44:58.652+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:44:58.683+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:44:58.709+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.516 seconds
[2023-10-15T23:45:28.960+0000] {processor.py:157} INFO - Started process (PID=2199) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:45:28.962+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:45:28.964+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:45:28.963+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:45:29.366+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:45:29.333+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:45:29.368+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:45:29.398+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.442 seconds
[2023-10-15T23:46:00.473+0000] {processor.py:157} INFO - Started process (PID=2210) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:46:00.475+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:46:00.476+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:46:00.476+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:46:00.954+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:46:00.914+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:46:00.956+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:46:00.983+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.513 seconds
[2023-10-15T23:46:31.114+0000] {processor.py:157} INFO - Started process (PID=2220) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:46:31.117+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:46:31.119+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:46:31.119+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:46:31.581+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:46:31.548+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:46:31.582+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:46:31.605+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.495 seconds
[2023-10-15T23:47:02.567+0000] {processor.py:157} INFO - Started process (PID=2231) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:47:02.569+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:47:02.571+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:47:02.571+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:47:03.002+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:47:02.972+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:47:03.004+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:47:03.054+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.491 seconds
[2023-10-15T23:47:33.592+0000] {processor.py:157} INFO - Started process (PID=2242) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:47:33.594+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:47:33.596+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:47:33.595+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:47:34.058+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:47:34.021+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:47:34.059+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:47:34.088+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.499 seconds
[2023-10-15T23:48:04.655+0000] {processor.py:157} INFO - Started process (PID=2253) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:48:04.656+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:48:04.659+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:48:04.658+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:48:05.113+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:48:05.079+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:48:05.114+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:48:05.142+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.490 seconds
[2023-10-15T23:48:35.699+0000] {processor.py:157} INFO - Started process (PID=2264) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:48:35.701+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:48:35.703+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:48:35.703+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:48:36.140+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:48:36.105+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:48:36.141+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:48:36.169+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.473 seconds
[2023-10-15T23:49:06.761+0000] {processor.py:157} INFO - Started process (PID=2275) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:49:06.763+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:49:06.765+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:49:06.764+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:49:07.228+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:49:07.207+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:49:07.229+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:49:07.261+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.504 seconds
[2023-10-15T23:49:37.810+0000] {processor.py:157} INFO - Started process (PID=2286) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:49:37.812+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:49:37.814+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:49:37.814+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:49:38.283+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:49:38.247+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:49:38.285+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:49:38.312+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.507 seconds
[2023-10-15T23:50:08.701+0000] {processor.py:157} INFO - Started process (PID=2297) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:50:08.703+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:50:08.705+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:50:08.705+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:50:09.167+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:50:09.135+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:50:09.168+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:50:09.192+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.494 seconds
[2023-10-15T23:50:39.294+0000] {processor.py:157} INFO - Started process (PID=2308) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:50:39.295+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:50:39.296+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:50:39.296+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:50:39.695+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:50:39.668+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:50:39.697+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:50:39.721+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.430 seconds
[2023-10-15T23:51:10.280+0000] {processor.py:157} INFO - Started process (PID=2320) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:51:10.282+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:51:10.285+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:51:10.284+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:51:10.749+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:51:10.715+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:51:10.750+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:51:10.773+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.497 seconds
[2023-10-15T23:51:41.333+0000] {processor.py:157} INFO - Started process (PID=2331) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:51:41.335+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:51:41.337+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:51:41.336+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:51:41.816+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:51:41.767+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:51:41.817+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:51:41.850+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.521 seconds
[2023-10-15T23:52:12.842+0000] {processor.py:157} INFO - Started process (PID=2342) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:52:12.844+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:52:12.846+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:52:12.846+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:52:13.274+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:52:13.244+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:52:13.276+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:52:13.297+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.459 seconds
[2023-10-15T23:52:43.867+0000] {processor.py:157} INFO - Started process (PID=2353) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:52:43.869+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:52:43.871+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:52:43.870+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:52:44.316+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:52:44.287+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:52:44.318+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:52:44.342+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.479 seconds
[2023-10-15T23:53:14.951+0000] {processor.py:157} INFO - Started process (PID=2364) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:53:14.953+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:53:14.955+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:53:14.955+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:53:15.405+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:53:15.371+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:53:15.406+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:53:15.428+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.480 seconds
[2023-10-15T23:53:46.461+0000] {processor.py:157} INFO - Started process (PID=2375) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:53:46.463+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:53:46.465+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:53:46.464+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:53:46.910+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:53:46.882+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:53:46.911+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:53:46.931+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.474 seconds
[2023-10-15T23:54:17.475+0000] {processor.py:157} INFO - Started process (PID=2386) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:54:17.476+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:54:17.478+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:54:17.477+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:54:17.925+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:54:17.892+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:54:17.927+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:54:17.954+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.484 seconds
[2023-10-15T23:54:48.080+0000] {processor.py:157} INFO - Started process (PID=2397) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:54:48.082+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:54:48.084+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:54:48.084+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:54:48.546+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:54:48.513+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:54:48.548+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:54:48.574+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.497 seconds
[2023-10-15T23:55:19.335+0000] {processor.py:157} INFO - Started process (PID=2407) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:55:19.336+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:55:19.338+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:55:19.338+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:55:19.762+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:55:19.736+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:55:19.763+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:55:19.791+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.460 seconds
[2023-10-15T23:55:50.331+0000] {processor.py:157} INFO - Started process (PID=2418) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:55:50.332+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:55:50.335+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:55:50.334+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:55:50.822+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:55:50.785+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:55:50.823+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:55:50.847+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.520 seconds
[2023-10-15T23:56:21.852+0000] {processor.py:157} INFO - Started process (PID=2429) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:56:21.854+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:56:21.857+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:56:21.856+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:56:22.313+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:56:22.280+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:56:22.314+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:56:22.353+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.504 seconds
[2023-10-15T23:56:52.921+0000] {processor.py:157} INFO - Started process (PID=2440) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:56:52.923+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:56:52.926+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:56:52.925+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:56:53.395+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:56:53.357+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:56:53.396+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:56:53.419+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.502 seconds
[2023-10-15T23:57:24.018+0000] {processor.py:157} INFO - Started process (PID=2451) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:57:24.020+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:57:24.022+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:57:24.022+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:57:24.478+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:57:24.446+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:57:24.480+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:57:24.498+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.483 seconds
[2023-10-15T23:57:55.054+0000] {processor.py:157} INFO - Started process (PID=2462) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:57:55.056+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:57:55.058+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:57:55.057+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:57:55.513+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:57:55.482+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:57:55.515+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:57:55.544+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.495 seconds
[2023-10-15T23:58:26.094+0000] {processor.py:157} INFO - Started process (PID=2473) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:58:26.096+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:58:26.098+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:58:26.098+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:58:26.565+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:58:26.534+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:58:26.567+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:58:26.594+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.504 seconds
[2023-10-15T23:58:57.115+0000] {processor.py:157} INFO - Started process (PID=2484) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:58:57.117+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:58:57.119+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:58:57.119+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:58:57.567+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:58:57.532+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:58:57.568+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:58:57.593+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.482 seconds
[2023-10-15T23:59:28.122+0000] {processor.py:157} INFO - Started process (PID=2502) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:59:28.124+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:59:28.126+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:59:28.126+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:59:28.589+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:59:28.554+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:59:28.591+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:59:28.617+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.498 seconds
[2023-10-15T23:59:59.206+0000] {processor.py:157} INFO - Started process (PID=2513) to work on /opt/airflow/dags/etl_dag.py
[2023-10-15T23:59:59.208+0000] {processor.py:829} INFO - Processing file /opt/airflow/dags/etl_dag.py for tasks to queue
[2023-10-15T23:59:59.209+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:59:59.209+0000] {dagbag.py:539} INFO - Filling up the DagBag from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:59:59.666+0000] {logging_mixin.py:151} INFO - [2023-10-15T23:59:59.637+0000] {dagbag.py:347} ERROR - Failed to import: /opt/airflow/dags/etl_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/dagbag.py", line 343, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 843, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_dag.py", line 5, in <module>
    from etl import read_csv, read_db, transform_spotify, grammys_transform_db, merge, load
  File "/opt/airflow/dags/etl.py", line 61, in <module>
    read_db()
  File "/opt/airflow/dags/etl.py", line 52, in read_db
    grammys_df = pd.read_sql(query, con=engine)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 633, in read_sql
    with pandasSQL_builder(con) as pandas_sql:
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 832, in pandasSQL_builder
    return SQLDatabase(con, schema, need_transaction)
  File "/home/airflow/.local/lib/python3.8/site-packages/pandas/io/sql.py", line 1539, in __init__
    con = self.exit_stack.enter_context(con.connect())
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 146, in _do_get
    self._dec_overflow()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/impl.py", line 143, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 691, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.8/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5432 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?
connection to server at "localhost" (::1), port 5432 failed: Cannot assign requested address
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-10-15T23:59:59.667+0000] {processor.py:841} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_dag.py
[2023-10-15T23:59:59.693+0000] {processor.py:179} INFO - Processing /opt/airflow/dags/etl_dag.py took 0.491 seconds
